{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSVp3WLll5jg/tMV0TT7h+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundation models for zero-shot detection and segmentation\n",
        "\n",
        "Based on [Ollama](https://github.com/ollama/ollama) project."
      ],
      "metadata": {
        "id": "hc5E2kAe2Gxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T9lBpOTx6Vx"
      },
      "outputs": [],
      "source": [
        "!curl -L https://ollama.com/download/ollama-linux-amd64 -o ollama\n",
        "!chmod +x ollama\n",
        "!./ollama pull llava\n",
        "#!cp ./ollama /usr/bin/ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"./ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Wait for a few seconds for Ollama to load!"
      ],
      "metadata": {
        "id": "HEwUusm_zUO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O xxx.jpg https://github.com/ant-nik/neural_network_course/blob/main/practice_2_data/video_1_fixed/image_001.jpg?raw=true"
      ],
      "metadata": {
        "id": "jzFdqEqE7IUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prompt.txt\n",
        "Find entities on the image.\n",
        "Split answer in two sections a LIST and a EXPLANATION.\n",
        "Put only detected object names to the LIST section.\n",
        "Put an explanation of the answer into the EXPLANATION section"
      ],
      "metadata": {
        "id": "0wpuEkm6Kw9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '{ \"model\": \"llava\", \"prompt\": \"'`cat prompt.txt`'\", \"images\": [\"'`base64 -w 0 /content/xxx.jpg`'\"], \"stream\": false}' > body.json"
      ],
      "metadata": {
        "id": "MijjTNOaBUcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://localhost:11434/api/generate --data-binary \"@body.json\""
      ],
      "metadata": {
        "id": "3CI_pQpD1hnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile step-2-prompt.txt\n",
        "Extract text between LIST and EXPLANATION sections and consider it as TEXT in the instruction below.\n",
        "Split answer in two parts: OUTPUT and INFO.\n",
        "Remove any enumeration symbols in the TEXT and place only one list entity per line to the OUTPUT section between START and END markers.\n",
        "Put any explanation of the answer to INFO section.\n",
        "\n",
        "LIST:\n",
        "1. Bottle\n",
        "2. Man\n",
        "3. Water bottle\n",
        "4. Rocks\n",
        "5. Dirt\n",
        "6. Trash bag\n",
        "7. Grass\n",
        "8. River\n",
        "9. Dogs (if any)\n",
        "10. Mountain\n",
        "\n",
        "EXPLANATION:\n",
        "The image shows a man outside in a natural environment. He appears to be bending over, possibly interacting with the ground or some kind of litter in his hands. There is a bottle near him, and it seems like he might be picking up trash from the area. The landscape suggests a rural or semi-rural setting with rocks, dirt, grass, and what could be a small river or stream visible in the background. Additionally, there appears to be a trash bag nearby, which supports the idea that the man is cleaning up litter.\n",
        "\n",
        "OUTPUT:\n"
      ],
      "metadata": {
        "id": "oymKK-HjQxLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo '{ \"model\": \"llama3.1\", \"prompt\": \"'`cat step-2-prompt.txt`'\", \"stream\": false}' > step-2-body.json"
      ],
      "metadata": {
        "id": "_Z_gdkksVnfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --data-binary \"@step-2-body.json\" -o step-2-result.txt http://localhost:11434/api/generate"
      ],
      "metadata": {
        "id": "wLjmLIhW7xHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"step-2-result.txt\", \"r\") as file:\n",
        "    step2_response = json.loads(file.read())\n",
        "print(step2_response[\"response\"])"
      ],
      "metadata": {
        "id": "9nTA2MFGZ1V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = [item for item in step2_response[\"response\"].split(\"START\")[1].split(\"END\")[0].split(\"\\n\") if not item=='']"
      ],
      "metadata": {
        "id": "XKHsQG7gfjA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "model_id = \"IDEA-Research/grounding-dino-base\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
        "\n",
        "image_url = \"https://drive.usercontent.google.com/u/0/uc?id=1Abxa12JrIk-R2iupQL0nEH5MWPWtD2H1&export=download\"\n",
        "image = Image.open(\"xxx.jpg\")"
      ],
      "metadata": {
        "id": "3-X2PYNUgZaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERY important: text queries need to be lowercased + end with a dot\n",
        "text = \" . \".join([f\"all {item}\" for item in objects]).lower() + '.'\n",
        "print(text)"
      ],
      "metadata": {
        "id": "xsTCl5eLsWtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "results = processor.post_process_grounded_object_detection(\n",
        "    outputs,\n",
        "    inputs.input_ids,\n",
        "    box_threshold=0.2,\n",
        "    text_threshold=0.2,\n",
        "    target_sizes=[image.size[::-1]]\n",
        ")\n",
        "results"
      ],
      "metadata": {
        "id": "qUGbiQKPtEA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "id": "vZQKC1XbhwQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import supervision\n",
        "import numpy\n",
        "\n",
        "\n",
        "box_annotator = supervision.BoxAnnotator()\n",
        "detections = supervision.Detections(\n",
        "    xyxy=results[0][\"boxes\"].numpy(),\n",
        "    class_id=numpy.ones(results[0][\"boxes\"].shape[0], dtype=int)\n",
        ") #, 2, 3, 4])#results[0][\"labels\"]\n",
        "\n",
        "\"\"\"\n",
        "labels = [\n",
        "    f\"{class_id} {confidence:0.2f}\"\n",
        "    for confidence, class_id, boxes in results\n",
        "]\n",
        "\"\"\"\n",
        "annotated_frame = box_annotator.annotate(scene=image.copy(),\n",
        "                                         detections=detections) #, labels=labels)\n",
        "\n",
        "%matplotlib inline\n",
        "supervision.plot_image(annotated_frame, (16, 16))"
      ],
      "metadata": {
        "id": "Fg-kM7s_iTjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OK1UnSpsjnS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}